# 🔎 RAG(Retrieval-Augmented Generation) 정리

![images](https://github.com/KoreaEva/HOL/raw/master/AzureOpenAI/part2%20RAG/images/01.RAG.png)

## 1. RAG란 무엇인가?

* **정의**: RAG는 \*\*검색(Retrieval)\*\*과 \*\*생성(Generation)\*\*을 결합한 AI 아키텍처.
* **목적**: LLM(대규모 언어 모델)이 최신 정보나 외부 지식을 반영해 **더 정확하고 풍부한 답변**을 생성하도록 돕는 것.
* **작동 원리**:

  1. 사용자의 질문 입력
  2. 관련 지식을 \*\*벡터 검색(semantic search)\*\*을 통해 찾아옴
  3. 찾아온 지식을 LLM에 함께 입력(context)
  4. LLM이 이를 바탕으로 답변 생성

  ![images](https://github.com/KoreaEva/HOL/raw/master/AzureOpenAI/part2%20RAG/images/02.overview.png)

즉, LLM의 **지식 한계를 보완**하고, **사실 기반(factual) 응답**을 강화하는 방식.

---

## 2. 왜 RAG가 필요한가?

1. **LLM의 한계 보완**

   * LLM은 학습 시점 이후의 최신 지식은 모름.
   * 환각(hallucination, 지어내기) 문제 발생 가능.
2. **업데이트 비용 절감**

   * 모델을 재학습하지 않고도 새로운 지식 반영 가능.
   * 예: 새 법률/정책, 최신 연구논문, 회사 내부 문서 검색.
3. **도메인 특화 활용**

   * 기업 내부 데이터, 논문, 제품 매뉴얼 등 **사내 전용 지식**을 반영 가능.

---

## 3. RAG 아키텍처 구조

보통 다음 단계로 구성됨:

1. **데이터 준비**

   * 문서 수집 → 텍스트 분할(chunking) → 임베딩(embedding) 변환
   * 벡터DB(예: Pinecone, Weaviate, FAISS, Milvus)에 저장
2. **검색 단계(Retrieval)**

   * 사용자의 질문을 임베딩 변환
   * 벡터DB에서 의미적으로 가장 유사한 문서 조각 검색
3. **생성 단계(Generation)**

   * 검색된 문서들을 LLM 입력(prompt)에 삽입
   * 문맥 기반으로 답변 생성
4. **후처리(Post-processing)**

   * 답변 요약, 출처 표시, 포맷팅 등 추가 처리

---

## 4. RAG의 주요 구성 요소

* **임베딩 모델(Embedding Model)**: 텍스트를 벡터(숫자 공간)로 변환
* **벡터 데이터베이스(Vector DB)**: 의미 검색을 위한 저장소
* **Retriever**: 질문과 관련 문서를 찾는 모듈
* **LLM**: 검색된 문서를 이용해 답변 생성
* **Prompt Engineering**: 문맥 삽입 방식 설계 (예: “다음 자료를 바탕으로만 답하시오”)

---

## 5. RAG의 장점

* 최신 정보 반영 (재학습 불필요)
* 특정 도메인에 특화 가능
* 환각 문제 감소
* 지식 관리 유연성 확보

---

## 6. RAG의 한계 및 과제

* **검색 품질에 따라 성능 좌우됨**

  * 벡터DB에 올바른 데이터가 없으면 답변도 부정확.
* **문서 전처리 중요성**

  * chunking 크기, 메타데이터 태깅 전략 등에 따라 결과 달라짐.
* **비용 문제**

  * 벡터DB + API 호출 비용 증가.
* **프라이버시/보안**

  * 민감 데이터 사용 시 권한 관리 필요.

---

## 7. RAG의 응용 사례

* **기업 지식관리**: 사내 문서 검색, 내부 Q\&A 시스템
* **법률/규제**: 최신 법령 반영한 챗봇
* **연구 지원**: 논문 검색 + 요약
* **전자상거래**: 제품 카탈로그 검색 + 설명
* **고객지원**: FAQ 자동화 및 맞춤형 응답

---

## 8. RAG 고급 기법

* **HyDE (Hypothetical Document Embeddings)**
  질문을 바탕으로 가상의 답변을 생성한 후 이를 검색 쿼리로 활용 → 검색 성능 향상.
* **Re-ranking**
  검색된 문서들을 다시 정렬하여 가장 적합한 것만 선택.
* **Multi-hop Retrieval**
  여러 단계의 검색을 거쳐 복잡한 질문 해결.
* **Tool-Augmented RAG**
  검색 외에도 계산기, API 호출 등 다른 도구와 결합.

---

## 9. RAG와 Fine-tuning 비교

* **Fine-tuning**: 모델 자체를 특정 데이터에 맞게 재학습 → 비용 크고 업데이트 어려움.
* **RAG**: 외부 지식 검색을 통해 최신성·유연성 확보 → 유지보수 쉬움.
* 결론: **변화가 잦은 지식 → RAG / 안정적이고 특화된 패턴 → Fine-tuning**

---

## 10. 앞으로의 발전 방향

* **RAG + AGI Agent**: 검색 후 도구 실행까지 가능.
* **멀티모달 RAG**: 텍스트뿐 아니라 이미지, 오디오, 영상 검색·활용.
* **Self-RAG**: 모델이 스스로 검색 필요 여부를 판단해 동적으로 검색.
* **지식 증류(Knowledge Distillation)**: RAG로 얻은 정보를 다시 모델에 주입해 학습.

---

👉 정리하면, RAG는 \*\*“LLM이 모든 걸 다 알 수는 없으니, 필요한 지식을 검색해서 보완하는 방식”\*\*이라고 할 수 있습니다.
즉, **사람이 구글 검색 후 글을 쓰는 것과 유사**한 개념이지요.